<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Pangff's Blog]]></title>
  <link href="http://www.pffair.com/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://www.pffair.com/"/>
  <updated>2021-05-04T23:07:36+08:00</updated>
  <id>http://www.pffair.com/</id>
  <author>
    <name><![CDATA[pangff]]></name>
    <email><![CDATA[540688649@qq.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[logspout-redis-elk]]></title>
    <link href="http://www.pffair.com/blog/2017/01/20/logspout-redis-elk/"/>
    <updated>2017-01-20T13:56:16+08:00</updated>
    <id>http://www.pffair.com/blog/2017/01/20/logspout-redis-elk</id>
    <content type="html"><![CDATA[<p>接上篇<a href="http://www.pffair.com/blog/2017/01/15/logspout-elk/">logspout-elk</a>.我们在上篇中完成了使用logspout收集docker日志并发送到logstash中。本文在之前的基础上在logspout到logstash之间添加了redis消息队列。</p>

<!--more-->


<h3>为什么要添加消息队列呢？</h3>

<p>参考ELK官方的文档<a href="https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html#deploying-message-queueing">Managing Throughput Spikes with Message Queueing</a>。当logstash索引消费速率低于传入速率时Logstash将会对传入事件进行限制，这样会使传入事件在数据源头堆积。添加消息队列不仅可以起到防止back pressure作用同时还可以放置数据丢失。当从消息队列消费数据失败我们还可以重新获取。在我们之前的架构中如果事件消费出现异常(比如我们的Elasticsearch出现问题)不能及时消费logstash获取到的日志时那么logstash做限入控制，数据将会在我们的container堆积，而此时如果我们container出现问题而被remove并且恰恰没有mount宿主机磁盘进行log存储的话那么我们之前的日志将会全部丢失，由此可见添加消息队列的重要性。至于为什么选用redis，只是简单了解到它的速度、易用性、和低资源需求比较好，最主要的原因还是相对 Kafka、RabbitMQ对它更熟悉。如果感兴趣可以对（redis、kafka、RabbitMQ）做一个简单对比和测试</p>

<h3>添加redis message broker</h3>

<p>由于采用redis message broker我们需要修改logstash的配置文件，所以整体的service创建我们重新走一遍，也算对之前内容的巩固</p>

<p>开始之前将全部的service、network删除</p>

<p><strong>删除service</strong></p>

<pre><code>docker service rm logstash \
elasticsearch proxy \
swarm-listener kibana
</code></pre>

<p><strong>删除network</strong></p>

<pre><code>docker network rm elk proxy
</code></pre>

<p><strong>创建elk overlay network</strong></p>

<pre><code>docker network create --driver overlay elk
</code></pre>

<p><strong>创建elasticsearch service</strong></p>

<pre><code>docker service create --name elasticsearch \
    --mode global \
    --network elk \
    -p 9200:9200 \
    -e ES_JAVA_OPTS="-Dmapper.allow_dots_in_name=true" \
    --constraint "node.labels.elk == yes" \
    --reserve-memory 500m \
    elasticsearch:2.4
</code></pre>

<p>需要注意的是</p>

<pre><code>-e ES_JAVA_OPTS="-Dmapper.allow_dots_in_name=true"
</code></pre>

<p>这个环境变量，使用它的目的是为了让Elasticsearch允许日志属性名称中有".&ldquo;，比如类似com.docker.node.id这样的属性名字，在Elasticsearch 2.X版本中是不允许的收集的日志中如果存在的话Elasticsearch会抛异常</p>

<pre><code>MapperParsingException[Field name [com.docker.node.id] cannot contain '.'] 
</code></pre>

<p><em>加个小备注：Elasticsearch的日志可以通过。</em> <strong>docker ps -a</strong>
<em>找到Elasticsearch的container id然后</em>
<strong>docker logs containerid</strong> <em>查看</em></p>

<p>具体关于点的问题可以参考<a href="https://www.elastic.co/guide/en/elasticsearch/reference/2.4/dots-in-names.html">dots-in-names</a>
当然也可以直接安装5.X的<a href="https://hub.docker.com/_/elasticsearch/">Elasticsearch</a>，5.0默认是支持的。我开始尝试安装5.0版本但是create service 会提示netwrok not found，不能创建成功，之后没有再尝试解决，感兴趣的话可以试试并且将elasticsearch 2.X版本和5.X做一个比较，5.x相对2.x的变化</p>

<p><strong>创建redis service,加入到elk network</strong></p>

<pre><code>docker service create --name redis \
--network elk \
-p 6379:6379  redis redis-server --requirepass redis
</code></pre>

<p>通过redis-server &ndash;requirepass 设置密码为redis。建议这里要设置密码，再实际操作时发现外部全局扫描redis时候并且可能运行CONFIG SET REQUIREPASS，会锁定redis运行实例，出现授权提示<a href="http://stackoverflow.com/questions/34115213/redis-raise-error-noauth-authentication-required-but-there-is-no-password-setti">参考</a></p>

<pre><code>redis (error) NOAUTH Authentication required.
</code></pre>

<p><strong>修改logstash配置文件</strong></p>

<pre><code>input {
  redis {
    host =&gt; "redis"
    data_type =&gt; "list"
    key =&gt; "logspout"
    codec =&gt; "json"
    password =&gt; "redis"
  }
}
filter {

}
output {
  elasticsearch {
    hosts =&gt; ["elasticsearch:9200"]
  }
}
</code></pre>

<p>这里需要注意的是key，它对应的是我们日志收集后存储到redis中的key，关于key的指定方式会在后面描述。filter部分我没有添加信息，真实环境根据需要在这里添加一些日志过滤处理，比如格式化、删除无用日志等等，感兴趣可以去看下。password为redis密码</p>

<p><strong>上传配置文件到节点主机</strong></p>

<pre><code>mkdir -p docker/logstash
cp conf/logstash.conf docker/logstash/logstash.conf
scp -r docker root@xxx.xxx.xxx.xxx:/
scp -r docker root@xxx.xxx.xxx.xxx:/
</code></pre>

<p><strong>创建logstash service</strong></p>

<pre><code>docker service create --name logstash \
    --mount "type=bind,source=/docker/logstash,target=/conf" \
    --mode global \
    --network elk \
    --constraint "node.labels.elk == yes" \
    -e LOGSPOUT=ignore \
    --reserve-memory 100m \
    mywebgrocer/logstash logstash -f /conf/logstash.conf
</code></pre>

<p><strong>创建proxy overlay network</strong></p>

<pre><code>docker network create --driver overlay proxy
</code></pre>

<p><strong>创建swarm-listener service</strong></p>

<pre><code>docker service create --name swarm-listener \
    --network proxy \
    --mount "type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock" \
    -e DF_NOTIF_CREATE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/reconfigure \
    -e DF_NOTIF_REMOVE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/remove \
    --constraint 'node.role==manager' \
    vfarcic/docker-flow-swarm-listener
</code></pre>

<p><strong>创建proxy service</strong></p>

<pre><code>docker service create --name proxy \
    -p 80:80 \
    -p 443:443 \
    --network proxy \
    -e MODE=swarm \
    -e LISTENER_ADDRESS=swarm-listener \
    vfarcic/docker-flow-proxy
</code></pre>

<p><strong>创建kibana service</strong></p>

<pre><code>docker service create --name kibana \
    --network elk \
    --network proxy \
    -e ELASTICSEARCH_URL=http://elasticsearch:9200 \
    --reserve-memory 50m \
    --label com.df.notify=true \
    --label com.df.distribute=true \
    --label com.df.servicePath=/app/kibana,/bundles,/elasticsearch \
    --label com.df.port=5601 \
    kibana:4.6
</code></pre>

<p><strong>比较重要的一步创建<a href="https://github.com/rtoma/logspout-redis-logstash">logspout-redis-logstash service</a></strong></p>

<pre><code>docker service create --name logspout \
    --network elk \
    --mode global \
    --mount "type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock" \
    rtoma/logspout-redis-logstash redis://redis?password=redis
</code></pre>

<p>其实logspout-redis-logstash是可以指定redis的key的如redis://redis?key=&ldquo;logspout"。因为不指定默认是logspout，我没有写默认就是logspout。所以这部分对应的就是前面logstash配置文件redis中的key值。password为redis密码</p>

<p><strong>我们自己的测试service</strong></p>

<pre><code>docker service create -p 3000:3000 --name hello-service pangff/hello-service:latest
</code></pre>

<p><strong>访问测试服务</strong></p>

<pre><code>open http://$(docker-machine ip es-swarm-1):3000/hello/123
</code></pre>

<p><strong>访问Kibana</strong></p>

<pre><code>open http://$(docker-machine ip es-swarm-1)/app/kibana
</code></pre>

<p>配置index，然后Discover，搜索hello结果如图（访问一次再看，会发现结果变多一条）</p>

<p><img src="http://www.pffair.com/images/65.png" alt="" /></p>

<p><em>如果访问Kibana后在创建logstash-**默认索引时候下面是灰色没有mapper时，说明elasticsearch没有收到日志。如果在创建logstash-默认索引时候可以创建但是Time-field name没有内容时候，说明日志格式问题或者Elasticsearch出现异常</em></p>

<h3>备注</h3>

<p><strong>查看redis container中logspout内容</strong></p>

<p>查看redis在哪个节点</p>

<pre><code>docker service ps redis
</code></pre>

<p>进入redis对应主机</p>

<pre><code>docker-machine ssh es-swarm-1
</code></pre>

<p>查看redis container id</p>

<pre><code>docker ps -a
</code></pre>

<p>找到redis容器id进入容器（2361aa523cd1是redis容器id）</p>

<pre><code>docker exec -it 2361aa523cd1 /bin/bash
</code></pre>

<p>访问redis</p>

<pre><code>redis-cli
</code></pre>

<p>查看key为logsput类型为list的数据前10条</p>

<pre><code>LRANGE logspout 0 10
</code></pre>

<h3>后续需要处理</h3>

<ul>
<li>是否有循环日志采集问题，比如统计到elasticsearch日志后，日志进入elasticsearch本身也会出现日志，这样造成循环</li>
<li>在正式环境下该日志处理架构的节点编排、network处理</li>
<li>elasticsearch的数据备份，磁盘扩容</li>
<li>该日志处理架构下，个个节点性能指标</li>
<li>该日志能否按需求收集相关日志,需要后续根据业务测试</li>
<li><a href="https://www.elastic.co/blog/kibana-4-video-tutorials-part-1">kibana</a>的深入学习</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[logspout-elk]]></title>
    <link href="http://www.pffair.com/blog/2017/01/15/logspout-elk/"/>
    <updated>2017-01-15T19:00:35+08:00</updated>
    <id>http://www.pffair.com/blog/2017/01/15/logspout-elk</id>
    <content type="html"><![CDATA[<p>接上篇<a href="http://www.pffair.com/blog/2017/01/12/elk-docker/">elk-docker</a>，在docker swarm环境部署好了elk，如何在swarm环境进行日志收集呢？本文使用logspout进行swarm中每个节点的日志收集并发送给logstash，logstash将日志存入elasticsearch中，kinana从elasticsearch读取日志信息进行展示。</p>

<!--more-->


<h3>创建logspout service</h3>

<p>使用以下命令进行logspout service创建</p>

<pre><code>docker service create --name logspout \
    --network elk \
    --mode global \
    --mount "type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock" \
    -e SYSLOG_FORMAT=rfc3164 \
    gliderlabs/logspout syslog://logstash:51415
</code></pre>

<p>创建global的logspout servive因为我们要在每一container上进行日志监控。将logspout service加入elk network，以便于和之前的logstash service进行通信。在服务启动的时候执行syslog://logstash:51415告诉logspout我们要使用syslog协议发送日志到在51415端口运行的logstash</p>

<p>查看logspout service状态</p>

<pre><code>docker service ps logspout
</code></pre>

<p><img src="http://www.pffair.com/images/64.png" alt="" /></p>

<p>等到logspout启动完成。我们用一个小例子看看logspout是否真的把日志发送给了logspout。<a href="https://github.com/MarshalW/hello-service">例子代码</a></p>

<p>创建基于restify的 <a href="(https://github.com/MarshalW/hello-service">hello-service</a>)</p>

<pre><code>docker service create -p 3000:3000 --name hello-service marshalw/hello-service:0.1.0
</code></pre>

<p>查看hello-service状态</p>

<pre><code>docker service ps hello-service
</code></pre>

<p>等待hello-service完全启动后，运行查下</p>

<pre><code>open http://$(docker-machine ip es-swarm-1):3000/hello/name
</code></pre>

<p>通过以下几个命令查看logstash对应container的日志</p>

<p>用以下查看当前节点全部container信息，并找到logstash对应 container id</p>

<pre><code>docker ps -a
</code></pre>

<p>用以下命令查看logstash日志信息（命令中65b7825aef55是上面找到的logstash container id）</p>

<pre><code>docker logs 65b7825aef55
</code></pre>

<p>可以发现以下日志条目。（因为我们的logstash是global的，每次日志发送不一定发送到了当前节点的logstash上来，要么通过多访问几次hello-service的方式，要么发现本节点没有就去另外的节点按相同方法找）</p>

<pre><code>{
           "message" =&gt; "restify listening at http://[::]:3000\n",
          "@version" =&gt; "1",
        "@timestamp" =&gt; "2017-01-15T12:09:18.000Z",
              "host" =&gt; "10.0.0.12",
          "priority" =&gt; 14,
     "timestamp8601" =&gt; "2017-01-15T12:09:18Z",
         "logsource" =&gt; "65b7825aef55",
           "program" =&gt; "hello-service.1.9zdbccrwv7q7yj8qpeg82kbr6",
               "pid" =&gt; "12204",
          "severity" =&gt; 6,
          "facility" =&gt; 1,
         "timestamp" =&gt; "2017-01-15T12:09:18Z",
    "facility_label" =&gt; "user-level",
    "severity_label" =&gt; "Informational"
}
</code></pre>

<p>说明我们hello-service的日志的确是发送到了logstash中。</p>

<p>运行</p>

<pre><code>open http://$(docker-machine ip es-swarm-1)/app/kibana
</code></pre>

<p>启动kibana，配置一个index可以在kibanna中看到我们hello-service的访问日志</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[elk-docker]]></title>
    <link href="http://www.pffair.com/blog/2017/01/12/elk-docker/"/>
    <updated>2017-01-12T23:18:34+08:00</updated>
    <id>http://www.pffair.com/blog/2017/01/12/elk-docker</id>
    <content type="html"><![CDATA[<p>本文接上篇<a href="http://www.pffair.com/blog/2017/01/11/ecs-docker-swarm/">ecs-docker-swarm</a>,在实现ESC docker swarm并添加es-swarm-2 label:elk=yes基础上部署elk(elasticsearch、logstash、kibana),其中elasticsearch部署到label中elk=yes的节点也就是es-swarm-2</p>

<!--more-->


<p>上文中的环境:</p>

<ul>
<li>es-swarm-1(manager)</li>
<li>es-swarm-2(worker,label:elk=yes)</li>
</ul>


<h3>创建elk overlay</h3>

<p> 创建以elk命名的overlay network，之后elk的通信将通过elk overlay</p>

<pre><code>eval $(docker-machine env swarm-1)

docker network create --driver overlay elk
</code></pre>

<h3>创建elasticsearch service</h3>

<p> 创建global的elasticsearch service，并且通过constraint(参考<a href="http://www.pffair.com/blog/2017/01/04/docker-constraint/">docker-constraint</a>)限制elasticsearch只能创建部署到label中elk＝yes的节点，也就是es-swarm-2节点</p>

<pre><code>docker service create --name elasticsearch \
    --mode global \
    --network elk \
    -p 9200:9200 \
    --constraint "node.labels.elk == yes" \
    --reserve-memory 500m \
    elasticsearch:2.4
</code></pre>

<p>因为后续的logstash servcie要依赖elasticsearch service所以要确保elasticsearch service完全启动后再去创建logstash。
如果是单独命令运行，只要用</p>

<pre><code>docker service ps elasticsearch
</code></pre>

<p>去检查下elasticsearch的CURRENT STATE是不是Running,不是就等会儿。通过这个命令我们还可以看到elasticsearch在个个节点部署状态,下图可以看到elasticsearch已经运行，而且只有在es-swarm-2上是Running。</p>

<p><img src="http://www.pffair.com/images/63.png" alt="" /></p>

<p>当然如果只是查看是否启动成功也可以</p>

<pre><code>open http://$(docker-machine ip es-swarm-1):9200
</code></pre>

<p>直接打开浏览器看是否可以访问</p>

<p>如果elk整个启动是在一个脚本中运行，那就需要做一个等待处理。有很多方法，我这里采用的是过滤获取docker service ps命令日志信息来判断（方法自己感觉不是很好，如果有好办法欢迎指正）</p>

<pre><code>
while true; do
    REPLICAS=$(docker service ps elasticsearch | grep es-swarm-2 | awk '{print $7}')
    echo "REPLICAS:"$REPLICAS
    A=$(docker service ps elasticsearch | grep es-swarm-2 | awk '{print $0}')
    echo $A
    if [[ $REPLICAS == "Running" ]]; then
        sleep 5
        echo "elasticsearch Running..."
        break
    else
        echo "Waiting for the elasticsearch service..."
        sleep 5
    fi
done
</code></pre>

<h3>创建logstash service</h3>

<p>添加logstash的配置文件，因为默认logstash的配置是</p>

<pre><code>input {
    stdin {}
    syslog {}
}
</code></pre>

<p>我们要将logstash收集的日志输出到elasticsearch所以要创建配置文件，并再通过 mount让logstash读取修改后的配置</p>

<p>在本地创建config/logstash.conf,并修改为以下内容</p>

<pre><code>input {
  syslog { port =&gt; 51415 }
}

output {
  elasticsearch {
    hosts =&gt; ["elasticsearch:9200"]
  }
  # Remove in production
  stdout {
    codec =&gt; rubydebug
  }
}
</code></pre>

<p>为了后面目录整齐，便于管理，我们创建统一docker目录。对于logstash的配置文件我们放到logstash目录中。并将docker目录传到两台ECS，也就是docker node上</p>

<pre><code>mkdir -p docker/logstash
cp conf/logstash.conf docker/logstash/logstash.conf
scp -r docker root@xxx.xxx.xxx.xxx:/
scp -r docker root@xxx.xxx.xxx.xxx:/
</code></pre>

<p>创建logstash service。logstash service是global的，目前并没有通过label去限制logstash（正式环境视情况定），加入elk network，我们将上传到ECS的目录bind到了logstash的conf下，指定logstash的conf配置在/conf/logstash.conf。配置环境变量LOGSPOUT=ignore为后续logspout做准备</p>

<pre><code>docker service create --name logstash \
    --mount "type=bind,source=/docker/logstash,target=/conf" \
    --mode global \
    --network elk \
    -e LOGSPOUT=ignore \
    --reserve-memory 100m \
    logstash:2.4 logstash -f /conf/logstash.conf
</code></pre>

<p>同样如果在一个脚本中，一样等待启动完成</p>

<pre><code>while true; do
    REPLICAS=$(docker service ps logstash | grep es-swarm-2 | awk '{print $7}')
    echo "REPLICAS:"$REPLICAS
     A=$(docker service ps logstash | grep es-swarm-2 | awk '{print $0}')
    echo $A
    if [[ $REPLICAS == "Running" ]]; then
        sleep 5
        echo "logstash Running..."
        break
    else
        echo "Waiting for the logstash service..."
        sleep 5
    fi
done
</code></pre>

<p>创建代理proxy overlay network。proxy用于代理与个个service间通信</p>

<pre><code>docker network create --driver overlay proxy
</code></pre>

<p>通过swarm-listener、docker-flow-proxy实现<a href="http://proxy.dockerflow.com/swarm-mode-auto/">swarm-mode-auto</a>。swarm-listener监控swarm service的创建销毁事件，当service创建销毁时自动发送请求给docker-flow-proxy进行重新配置(<a href="http://proxy.dockerflow.com/usage/#reconfigure">reconfigure</a>)。</p>

<p>创建swarm-listener service。-e环境变量意义部分参考文章<a href="http://proxy.dockerflow.com/swarm-mode-auto/">swarm-mode-auto</a></p>

<pre><code>docker service create --name swarm-listener \
    --network proxy \
    --mount "type=bind,source=/var/run/docker.sock,target=/var/run/docker.sock" \
    -e DF_NOTIF_CREATE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/reconfigure \
    -e DF_NOTIF_REMOVE_SERVICE_URL=http://proxy:8080/v1/docker-flow-proxy/remove \
    --constraint 'node.role==manager' \
    vfarcic/docker-flow-swarm-listener
</code></pre>

<p>创建proxy service。80 http请求，443 https请求。外部请求通过proxy代理到目标service</p>

<pre><code>docker service create --name proxy \
    -p 80:80 \
    -p 443:443 \
    --network proxy \
    -e MODE=swarm \
    -e LISTENER_ADDRESS=swarm-listener \
    vfarcic/docker-flow-proxy
</code></pre>

<p>一个shell脚本运行，确保服务启动。通过service ls的 replicas=1/1 确保swarm-listener 和 proxy启动完成</p>

<pre><code>while true; do
    REPLICAS=$(docker service ls | grep swarm-listener | awk '{print $3}')
    REPLICAS_NEW=$(docker service ls | grep swarm-listener | awk '{print $4}')
    if [[ $REPLICAS == "1/1" || $REPLICAS_NEW == "1/1" ]]; then
        break
    else
        echo "Waiting for the swarm-listener service..."
        sleep 5
    fi
done

while true; do
    REPLICAS=$(docker service ls | grep proxy | awk '{print $3}')
    REPLICAS_NEW=$(docker service ls | grep proxy | awk '{print $4}')
    if [[ $REPLICAS == "1/1" || $REPLICAS_NEW == "1/1" ]]; then
        break
    else
        echo "Waiting for the proxy service..."
        sleep 5
    fi
done
</code></pre>

<p>创建kibana service 并等待完成。kibana中的label作用参考<a href="http://proxy.dockerflow.com/swarm-mode-auto/">swarm-mode-auto</a></p>

<pre><code>
docker service create --name kibana \
    --network elk \
    --network proxy \
    -e ELASTICSEARCH_URL=http://elasticsearch:9200 \
    --reserve-memory 50m \
    --label com.df.notify=true \
    --label com.df.distribute=true \
    --label com.df.servicePath=/app/kibana,/bundles,/elasticsearch \
    --label com.df.port=5601 \
    kibana:4.6
</code></pre>

<p>一个脚本运行时，确保服务启动</p>

<pre><code>
while true; do
    REPLICAS=$(docker service ls | grep proxy | awk '{print $3}')
    REPLICAS_NEW=$(docker service ls | grep proxy | awk '{print $4}')
    if [[ $REPLICAS == "1/1" || $REPLICAS_NEW == "1/1" ]]; then
        break
    else
        echo "Waiting for the proxy service..."
        sleep 5
    fi
done
</code></pre>

<p>访问kibana</p>

<pre><code>open http://$(docker-machine ip es-swarm-1)/app/kibana
</code></pre>

<p>至此，在swarm mode 中部署elk完成。
完整脚本参考<a href="https://github.com/pangff/docker-swarm-sh/blob/master/scripts/dm-swarm-services-elk.sh">dm-swarm-services-elk</a></p>

<p><a href="">后续将通过logspout进行日志采集并发送到logstash再到elasticsearch，并通过kibana进行查看的例子 >>></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ecs-docker-swarm]]></title>
    <link href="http://www.pffair.com/blog/2017/01/11/ecs-docker-swarm/"/>
    <updated>2017-01-11T21:25:06+08:00</updated>
    <id>http://www.pffair.com/blog/2017/01/11/ecs-docker-swarm</id>
    <content type="html"><![CDATA[<p>接前篇<a href="http://www.pffair.com/blog/2017/01/11/docker-on-ecs/">安装docker到ECS</a>，接下来实现在本机使用docker machine实现swarm创建和节点管理。</p>

<ul>
<li>本机docker machine</li>
<li>es-swarm-1(manager)</li>
<li>es-swarm-2(worker)</li>
</ul>


<!--more-->


<h3>准备工作</h3>

<ul>
<li>为已安装docker的ECS es-swarm-1实例开启端口

<ul>
<li>TCP端口2377用于集群管理通信</li>
<li>TCP和UDP端口7946用于节点之间的通信</li>
<li>TCP和UDP端口4789用于overlay网络交互</li>
</ul>
</li>
<li>对已安装docker的ECS es-swarm-1实例进行自定义镜像制作，阿里的管理控制台一键完成</li>
<li>再购买一台ECS实例，采用刚刚制作的自定义镜像，命名为es-swarm-2</li>
<li>在本机安装docker machine,我的系统是Mac OS，直接下载dmg安装</li>
<li>将本机公钥配置到es-swarm-1、es-swarm-2的authorized_key中，确保本机可以无密登录es-swarm-1、es-swarm-2。可以通过ssh-copy-id方便的配置</li>
</ul>


<pre><code> ssh-copy-id root@xxx.xxx.xxx.xxx
</code></pre>

<p> 把xxx.xxx.xxx.xxx分别替换为es-swarm-1、es-swarm-2的ip即可</p>

<h3>关联远程node创建machine实例</h3>

<p>通过docker-machine的<a href="https://docs.docker.com/machine/drivers/generic/">generic</a>实现分别关联远程es-swarm-1、es-swarm-2创建machine实例</p>

<pre><code>docker-machine -D create \
--driver generic \
--generic-ip-address xxx.xxx.xxx.xxx \
--generic-ssh-user root es-swarm-1

docker-machine -D create \
--driver generic \
--generic-ip-address xxx.xxx.xxx.xxx \
--generic-ssh-user root es-swarm-2
</code></pre>

<p>将xxx.xxx.xxx.xxx分别替换为es-swarm-1、es-swarm-2的ip。确认下是否成功</p>

<pre><code> docker-machine ls
</code></pre>

<p><img src="http://www.pffair.com/images/62.png" alt="" /></p>

<h3>swarm初始化，es-swarm-1做manager</h3>

<pre><code>eval $(docker-machine env es-swarm-1)

docker swarm init \
  --advertise-addr $(docker-machine ip es-swarm-1)
</code></pre>

<h3>将es-swarm-2加入</h3>

<pre><code>TOKEN=$(docker swarm join-token -q manager)

eval $(docker-machine env es-swarm-2)

docker swarm join \
        --token $TOKEN \
        --advertise-addr $(docker-machine ip es-swarm-2) \
        $(docker-machine ip es-swarm-1):2377
</code></pre>

<p>至此创建完成，查看下节点状态</p>

<pre><code>docker node ls
</code></pre>

<p><img src="http://www.pffair.com/images/61.png" alt="" /></p>

<h3>为es-swarm-2添加elk label为后续ELK部署做准备</h3>

<pre><code>docker node update \
        --label-add elk=yes \
        es-swarm-2
</code></pre>

<p>整个流程我们可以放到一个shell脚本中，一次完成</p>

<p>完整脚本参考<a href="https://github.com/pangff/docker-swarm-sh/blob/master/scripts/dm-swarm.sh">dm-swarm.sh</a></p>

<p><a href="http://www.pffair.com/blog/2017/01/12/elk-docker/">后续，部署ELK，并指定ELK中 elasticsearch部署到指定的es-swarm-2节点 >>></a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[安装docker到ECS]]></title>
    <link href="http://www.pffair.com/blog/2017/01/11/docker-on-ecs/"/>
    <updated>2017-01-11T16:48:04+08:00</updated>
    <id>http://www.pffair.com/blog/2017/01/11/docker-on-ecs</id>
    <content type="html"><![CDATA[<p>本文不采用阿里云管理平台的配置，安装docker到ECS</p>

<!--more-->


<h3>准备工作</h3>

<p>购买一台阿里ECS，只是为了测试可以使用按量付费并最低配置,系统ubuntu 16.04。注意版本很重要，docker有限支持ubuntu的版本，命名为es-swarm-1</p>

<h3>安装docker</h3>

<p>安装参考<a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/">Install Docker on Ubuntu</a> (当然也可以通过docker-machine去安装)，本文采用<a href="https://docs.docker.com/engine/installation/linux/ubuntulinux/">Install Docker on Ubuntu</a>方式。</p>

<p>购买启动后，ssh到购买的ECS上</p>

<pre><code>ssh root@xxx.xxx.xxx.xxx
</code></pre>

<p>更新APT ，确保APT使用https方法，并且已安装CA证书</p>

<pre><code>$ sudo apt-get update
$ sudo apt-get install apt-transport-https ca-certificates
</code></pre>

<p>添加新的GPG密钥</p>

<pre><code>$ sudo apt-key adv \
               --keyserver hkp://ha.pool.sks-keyservers.net:80 \
               --recv-keys 58118E89F3A912897C070ADBF76221572C52609D
</code></pre>

<p>配置docker资源库。“deb <a href="https://apt.dockerproject.org/repo">https://apt.dockerproject.org/repo</a> ubuntu-xenial main” 部分根据根据不同Ubuntu系统版本使用不同配置</p>

<pre><code>$ echo "deb https://apt.dockerproject.org/repo ubuntu-xenial main" | sudo tee /etc/apt/sources.list.d/docker.list
</code></pre>

<p>更新APT</p>

<pre><code>$ sudo apt-get update 
</code></pre>

<p>在这步会出错，因为阿里将<a href="https://apt.dockerproject.org/repo">https://apt.dockerproject.org/repo</a> 定向到了<a href="http://mirrors.aliyun.com/">http://mirrors.aliyun.com/</a> 。</p>

<p>解决方法：</p>

<ul>
<li>将 /etc/apt/apt.conf
里 Acquire::<a href="http::Proxy">http::Proxy</a> &ldquo;<a href="http://mirrors.aliyun.com/">http://mirrors.aliyun.com/</a>&rdquo;; 注释掉</li>
<li>当然也可以配置到阿里的镜像源，速度快还省流量，具体没有实践</li>
</ul>


<p>配置完成后再次更新APT</p>

<pre><code>$ sudo apt-get update 
</code></pre>

<p>验证下APT是否从正确镜像库拉取</p>

<pre><code>$  apt-cache policy docker-engine

 docker-engine:
    Installed: 1.12.2-0~trusty
    Candidate: 1.12.2-0~trusty
    Version table:
   *** 1.12.2-0~trusty 0
          500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages
          100 /var/lib/dpkg/status
       1.12.1-0~trusty 0
          500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages
       1.12.0-0~trusty 0
          500 https://apt.dockerproject.org/repo/ ubuntu-trusty/main amd64 Packages
</code></pre>

<p>对于Ubuntu Trusty，Wily和Xenial，使用aufs存储驱动程序需要安装linux-image-extra- *内核包。</p>

<pre><code>$ sudo apt-get update
$ sudo apt-get install linux-image-extra-$(uname -r) linux-image-extra-virtual
</code></pre>

<p>终于可以安装docker了，安装最新版本</p>

<pre><code>$ sudo apt-get update
$ sudo apt-get install docker-engine
$ sudo service docker start
</code></pre>

<p>自己跑一个image测试下，docker安装完成，<a href="http://www.baidu.com">后面进行两台ECS实现docker swarm mode</a></p>

<ul>
<li>es-swarm-1(manager)</li>
<li>es-swarm-2(worker)</li>
</ul>


<p><a href="http://www.pffair.com/blog/2017/01/11/ecs-docker-swarm/">ECS docker swarm mode  >>></a></p>
]]></content>
  </entry>
  
</feed>
